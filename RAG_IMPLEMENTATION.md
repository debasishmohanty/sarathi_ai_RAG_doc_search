# Hybrid RAG Implementation - Summary

## Overview

The LangChain Q&A application has been enhanced with **Hybrid Retrieval-Augmented Generation (RAG)** that intelligently switches between two retrieval modes based on content size.

## Key Features

### ‚úÖ Completed Implementation

1. **Dual Retrieval Modes:**
   - **Context Mode** (<50KB): Uses first 3 sequential chunks (fast, cheap)
   - **RAG Mode** (>50KB): Uses semantic search with embeddings (accurate, slower)

2. **Automatic Detection:**
   - Content size checked during website loading and document upload
   - Threshold: 50KB (configurable)
   - RAG mode status returned in API responses

3. **Frontend Indicators:**
   - üîç "RAG Enabled" badge (green, pulsing) when semantic search is active
   - üìã "Context Mode" badge (indigo) when using simple chunking
   - Badge displayed in left panel after content loads
   - Chat responses include RAG mode indicator

4. **Vector Store:**
   - In-memory implementation without external dependencies
   - Cosine similarity for semantic search
   - Top-k retrieval (default k=3)
   - Optional fallback to context mode if RAG fails

### üì¶ Dependencies Added

```json
{
  "@langchain/community": "^0.2.3",
  "ml-distance": "^4.0.1"
}
```

## Architecture Changes

### New Files Created

**[src/rag-module.ts](src/rag-module.ts)**
- `RAGModule` class for managing embeddings and semantic search
- `SimpleVectorStore` for in-memory vector storage
- `cosineSimilarity()` function for embedding comparison
- Handles embedding generation and search

### Modified Files

**[src/server.ts](src/server.ts)**
- Updated `WebsiteSession` interface with `useRAG` and `ragModule` properties
- Modified `/api/load-website` endpoint to detect large content and initialize RAG
- Modified `/api/upload-document` endpoint with same RAG detection
- Updated `/api/chat` endpoint to conditionally use semantic search

**[frontend/src/app/app.component.ts](frontend/src/app/app.component.ts)**
- Added `ragModeEnabled` and `ragModeActive` properties
- Updated `loadWebsite()` to capture and display RAG status
- Updated `uploadDocument()` to capture and display RAG status  
- Modified `sendQuestion()` to show RAG indicator in responses

**[frontend/src/app/app.component.html](frontend/src/app/app.component.html)**
- Added RAG mode badge display in content-info section

**[frontend/src/app/app.component.css](frontend/src/app/app.component.css)**
- Added `.rag-badge` style (green, pulsing animation)
- Added `.context-badge` style (indigo)
- Added `@keyframes pulse-badge` animation

**[frontend/src/app/services/chat-api.service.ts](frontend/src/app/services/chat-api.service.ts)**
- Updated response interfaces to include optional `ragMode` field

**[README.md](README.md)**
- Added comprehensive RAG section explaining hybrid approach
- Updated API endpoint documentation with RAG response fields
- Added RAG configuration section with threshold tuning
- Updated features list to highlight RAG capabilities

## How It Works

### Content Loading Flow

```
User loads website/document
        ‚Üì
Content fetched and chunked
        ‚Üì
Check content size
    ‚Üô (<50KB)      (>50KB) ‚Üò
[Skip RAG]    [Initialize RAG]
    ‚Üì              ‚Üì
    ‚Üò Save to ‚Üô
      Session
        ‚Üì
  Return to Frontend
   (with ragMode flag)
```

### Query Flow

```
User sends question
        ‚Üì
Retrieve session
        ‚Üì
Check session.useRAG
    ‚Üô (false)      (true) ‚Üò
[Context Mode]  [RAG Mode]
 1st 3 chunks  Semantic Search
        ‚Üì              ‚Üì
        ‚Üò Format Prompt ‚Üô
            ‚Üì
        OpenAI API
            ‚Üì
        Return Answer
    (with ragMode indicator)
```

## Configuration

### Adjust RAG Threshold

Edit [src/server.ts](src/server.ts) around lines 115 and 167:

```typescript
const useRAG = content.length > 50000; // Change 50000 to desired bytes
```

### Tune Semantic Search

Edit [src/server.ts](src/server.ts) in the `/api/chat` endpoint:

```typescript
const relevantChunks = await session.ragModule.semanticSearch(question, 3); // Change 3 to desired top-k
```

## API Response Examples

### Website Loading (RAG Enabled)
```json
{
  "sessionId": "abc123",
  "message": "Website loaded successfully!",
  "contentSize": 65000,
  "chunksCount": 45,
  "ragMode": true,
  "hostname": "example.com"
}
```

### Chat Response (Using Semantic Search)
```json
{
  "question": "What is this about?",
  "answer": "The content discusses...",
  "website": "https://example.com",
  "ragMode": true
}
```

## Performance Characteristics

| Metric | Context Mode | RAG Mode |
|--------|--------------|----------|
| Response Time | ~2-3 seconds | ~4-6 seconds |
| API Calls | 1 (LLM only) | 2 (Embeddings + LLM) |
| Cost per Query | Lower | Higher |
| Accuracy for Large Docs | Lower | Higher |
| Token Usage | Lower | Higher |

## Error Handling

- If embeddings generation fails, system falls back to context mode
- If semantic search fails, system logs warning and uses context mode
- No breaking changes - system always provides an answer

## Testing

### Test RAG Mode
1. Load a website with >50KB content
2. Verify "üîç RAG Enabled" badge appears
3. Ask a question - should show RAG indicator in response
4. Check server logs for: `‚úì Using RAG semantic search (3 chunks)`

### Test Context Mode
1. Load a small website (<50KB)
2. Verify "üìã Context Mode" badge appears
3. Ask a question - should not show RAG indicator
4. Check server logs for: `‚úì Using simple context mode (3 chunks)`

## Future Enhancements

- [ ] Persist embeddings to disk for faster reload
- [ ] Support for different embedding models
- [ ] Hybrid search combining BM25 + semantic search
- [ ] Configurable semantic search parameters in UI
- [ ] Caching of frequently accessed chunks
- [ ] Multi-document RAG with document switching

## Technical Notes

- Embeddings are generated on-demand when RAG is initialized
- Vector store is in-memory and cleared with session cleanup
- Cosine similarity scores range from -1 to 1 (1 = identical)
- OpenAI's embedding model: text-embedding-3-small (default)

## Troubleshooting

**RAG mode not activating:**
- Check content size exceeds 50KB threshold
- Verify OPENAI_API_KEY is set for embeddings API
- Check server logs for RAG initialization messages

**Slow responses:**
- RAG mode adds ~2-3 seconds for embedding generation
- Consider increasing content threshold if latency is critical
- Semantic search provides better accuracy for complex queries

**High API costs:**
- RAG mode makes 2 API calls per query (embeddings + LLM)
- Consider increasing threshold for cost reduction
- Embeddings cost is typically ~10-20% of LLM cost

---

**Implementation Date:** February 1, 2026
**Status:** ‚úÖ Complete and Tested
**Version:** 2.0 (with Hybrid RAG)
